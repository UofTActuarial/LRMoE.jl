# LRMoE.jl

**LRMoE.jl** is an implementation of the Logit-Reduced Mixture-of-Experts model in `julia`.
This package is introduced in [Tseung et al. 2021](https://www.cambridge.org/core/journals/annals-of-actuarial-science/article/abs/lrmoejl-a-software-package-for-insurance-loss-modelling-using-mixture-of-experts-regression-model/18B8F5C17733C4DBAF2F921E08372DD8).

To install the stable version of the package, simply type the following in the `julia` REPL:
```julia
] add LRMoE
```

To install the latest version, type the following in the `julia` REPL:
```julia
] add https://github.com/sparktseung/LRMoE.jl
```

The website of full documentation is [here](https://work.sparktseung.com/LRMoE.jl/dev/).



